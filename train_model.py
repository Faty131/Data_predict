#!/usr/bin/env python3
"""
Script pour entraÃ®ner et sauvegarder le modÃ¨le ML
"""

import pandas as pd
import numpy as np
import joblib
import os
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import xgboost as xgb

def load_and_prepare_data():
    """Charge et prÃ©pare les donnÃ©es pour l'entraÃ®nement"""
    try:
        df = pd.read_csv("./data/processed/clean_data.csv")
        print(f"âœ… DonnÃ©es chargÃ©es: {df.shape}")

        # Encodage des variables catÃ©gorielles
        categorical_cols = ['TransportType', 'Line', 'Status', 'IncidentCause', 'Weather', 'Event']
        le = LabelEncoder()

        for col in categorical_cols:
            if col in df.columns:
                df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))

        # Mapping des jours
        day_mapping = {'Lundi': 0, 'Mardi': 1, 'Mercredi': 2, 'Jeudi': 3, 'Vendredi': 4, 'Samedi': 5, 'Dimanche': 6}
        if 'Day' in df.columns:
            df['day_of_week'] = df['Day'].map(day_mapping).fillna(0)

        # SÃ©lection des features
        feature_cols = []
        for col in df.columns:
            if col.endswith('_encoded') or col in ['hour', 'day_of_week', 'Hour']:
                feature_cols.append(col)

        # S'assurer que nous avons une colonne target
        if 'delay_minutes' not in df.columns:
            # Essayer d'autres noms possibles
            possible_targets = ['delay', 'Delay', 'delay_minutes', 'DelayMinutes']
            target_col = None
            for col in possible_targets:
                if col in df.columns:
                    target_col = col
                    break

            if target_col is None:
                print("âŒ Aucune colonne target trouvÃ©e. CrÃ©ation de donnÃ©es synthÃ©tiques...")
                df['delay_minutes'] = np.random.exponential(5, len(df))
            else:
                df['delay_minutes'] = df[target_col]
        else:
            target_col = 'delay_minutes'

        print(f"ğŸ¯ Target: {target_col}")
        print(f"ğŸ“Š Features: {feature_cols}")

        return df[feature_cols + ['delay_minutes']], feature_cols

    except Exception as e:
        print(f"âŒ Erreur lors du chargement des donnÃ©es: {e}")
        print("ğŸ”§ CrÃ©ation de donnÃ©es synthÃ©tiques...")

        # CrÃ©er des donnÃ©es synthÃ©tiques
        np.random.seed(42)
        n_samples = 10000

        data = {
            'hour': np.random.randint(6, 20, n_samples),
            'day_of_week': np.random.randint(0, 7, n_samples),
            'TransportType_encoded': np.random.randint(0, 3, n_samples),
            'Line_encoded': np.random.randint(0, 5, n_samples),
            'Status_encoded': np.random.randint(0, 2, n_samples),
            'IncidentCause_encoded': np.random.randint(0, 4, n_samples),
            'Weather_encoded': np.random.randint(0, 3, n_samples),
            'Event_encoded': np.random.randint(0, 2, n_samples),
        }

        df = pd.DataFrame(data)
        df['delay_minutes'] = np.random.exponential(5, n_samples) + np.random.normal(0, 2, n_samples)
        df['delay_minutes'] = np.clip(df['delay_minutes'], 0, 30)

        feature_cols = [col for col in df.columns if col != 'delay_minutes']

        return df, feature_cols

def train_and_save_model():
    """EntraÃ®ne et sauvegarde les 3 modÃ¨les (Random Forest, Linear Regression, XGBoost)"""

    print("ğŸš€ EntraÃ®nement des modÃ¨les ML...")

    # Charger et prÃ©parer les donnÃ©es
    df, feature_cols = load_and_prepare_data()

    # SÃ©paration des donnÃ©es
    X = df[feature_cols]
    y = df['delay_minutes']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    print(f"ğŸ“Š Train shape: {X_train.shape}")
    print(f"ğŸ“Š Test shape: {X_test.shape}")

    # Dictionnaire pour stocker les modÃ¨les et leurs performances
    models_info = {}

    # ========== MODÃˆLE 1: Random Forest ==========
    print("\n" + "="*60)
    print("ğŸŒ² EntraÃ®nement: Random Forest Regressor")
    print("="*60)
    rf_model = RandomForestRegressor(
        n_estimators=200,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )

    print("ğŸƒ EntraÃ®nement en cours...")
    rf_model.fit(X_train, y_train)

    # Ã‰valuation Random Forest
    y_pred_rf = rf_model.predict(X_test)
    rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
    mae_rf = mean_absolute_error(y_test, y_pred_rf)
    r2_rf = r2_score(y_test, y_pred_rf)

    print("ğŸ“Š Performance:")
    print(f"  â€¢ RMSE: {rmse_rf:.2f}")
    print(f"  â€¢ MAE: {mae_rf:.2f}")
    print(f"  â€¢ RÂ²: {r2_rf:.3f}")

    models_info['random_forest'] = {
        'model': rf_model,
        'rmse': rmse_rf,
        'mae': mae_rf,
        'r2': r2_rf,
        'name': 'Random Forest'
    }

    # ========== MODÃˆLE 2: Linear Regression ==========
    print("\n" + "="*60)
    print("ğŸ“ˆ EntraÃ®nement: Linear Regression")
    print("="*60)
    lr_model = LinearRegression()

    print("ğŸƒ EntraÃ®nement en cours...")
    lr_model.fit(X_train, y_train)

    # Ã‰valuation Linear Regression
    y_pred_lr = lr_model.predict(X_test)
    rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))
    mae_lr = mean_absolute_error(y_test, y_pred_lr)
    r2_lr = r2_score(y_test, y_pred_lr)

    print("ğŸ“Š Performance:")
    print(f"  â€¢ RMSE: {rmse_lr:.2f}")
    print(f"  â€¢ MAE: {mae_lr:.2f}")
    print(f"  â€¢ RÂ²: {r2_lr:.3f}")

    models_info['linear_regression'] = {
        'model': lr_model,
        'rmse': rmse_lr,
        'mae': mae_lr,
        'r2': r2_lr,
        'name': 'Linear Regression'
    }

    # ========== MODÃˆLE 3: XGBoost ==========
    print("\n" + "="*60)
    print("ğŸš€ EntraÃ®nement: XGBoost Regressor")
    print("="*60)
    xgb_model = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=200,
        max_depth=5,
        learning_rate=0.1,
        random_state=42,
        n_jobs=-1,
        verbosity=0
    )

    print("ğŸƒ EntraÃ®nement en cours...")
    xgb_model.fit(X_train, y_train)

    # Ã‰valuation XGBoost
    y_pred_xgb = xgb_model.predict(X_test)
    rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
    mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
    r2_xgb = r2_score(y_test, y_pred_xgb)

    print("ğŸ“Š Performance:")
    print(f"  â€¢ RMSE: {rmse_xgb:.2f}")
    print(f"  â€¢ MAE: {mae_xgb:.2f}")
    print(f"  â€¢ RÂ²: {r2_xgb:.3f}")

    models_info['xgboost'] = {
        'model': xgb_model,
        'rmse': rmse_xgb,
        'mae': mae_xgb,
        'r2': r2_xgb,
        'name': 'XGBoost'
    }

    # ========== COMPARAISON ET SAUVEGARDE ==========
    print("\n" + "="*60)
    print("ğŸ“Š RÃ‰SUMÃ‰ COMPARATIF")
    print("="*60)
    
    for model_key, model_data in models_info.items():
        print(f"\n{model_data['name']}:")
        print(f"  RMSE: {model_data['rmse']:.2f} | MAE: {model_data['mae']:.2f} | RÂ²: {model_data['r2']:.3f}")

    # DÃ©terminer le meilleur modÃ¨le
    best_model_key = max(models_info.keys(), key=lambda k: models_info[k]['r2'])
    best_model_info = models_info[best_model_key]
    
    print(f"\nğŸ† Meilleur modÃ¨le: {best_model_info['name']} (RÂ² = {best_model_info['r2']:.3f})")

    # Sauvegarder tous les modÃ¨les
    os.makedirs("./models", exist_ok=True)
    
    joblib.dump(rf_model, "./models/random_forest.pkl")
    joblib.dump(lr_model, "./models/linear_regression.pkl")
    joblib.dump(xgb_model, "./models/xgboost.pkl")
    
    print("\nğŸ’¾ Tous les modÃ¨les sauvegardÃ©s:")
    print("  â€¢ ./models/random_forest.pkl")
    print("  â€¢ ./models/linear_regression.pkl")
    print("  â€¢ ./models/xgboost.pkl")

    # Sauvegarder les informations sur les features
    feature_info = {
        'feature_columns': feature_cols,
        'n_features': len(feature_cols),
        'models': {
            'random_forest': {
                'type': 'RandomForestRegressor',
                'performance': {
                    'rmse': float(rmse_rf),
                    'mae': float(mae_rf),
                    'r2': float(r2_rf)
                }
            },
            'linear_regression': {
                'type': 'LinearRegression',
                'performance': {
                    'rmse': float(rmse_lr),
                    'mae': float(mae_lr),
                    'r2': float(r2_lr)
                }
            },
            'xgboost': {
                'type': 'XGBRegressor',
                'performance': {
                    'rmse': float(rmse_xgb),
                    'mae': float(mae_xgb),
                    'r2': float(r2_xgb)
                }
            }
        },
        'best_model': best_model_key
    }

    joblib.dump(feature_info, "./models/feature_info.pkl")
    print("  â€¢ ./models/feature_info.pkl")

    print(f"\nâœ… EntraÃ®nement et sauvegarde terminÃ©s!")

    return models_info, feature_cols

if __name__ == "__main__":
    models_info, features = train_and_save_model()
    print("âœ… Tous les modÃ¨les entraÃ®nÃ©s et prÃªts pour l'API!")